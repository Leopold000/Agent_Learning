// 03_chat_stream.js
// èŠå¤©æµå¼è¾“å‡º
import { ChatOllama } from "@langchain/ollama";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { InMemoryChatMessageHistory } from "@langchain/core/chat_history";
import {
  RunnableSequence,
  RunnableWithMessageHistory,
} from "@langchain/core/runnables";

// 1. æœ¬åœ°æ¨¡å‹
const model = new ChatOllama({
  model: "llama3.1:8b",
  baseUrl: "http://localhost:11434",
});

// 2. prompt
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ AI åŠ©æ‰‹ã€‚"],
  ["placeholder", "{history}"],
  ["human", "{input}"],
]);

// 3. ä¸» chain
const chain = RunnableSequence.from([prompt, model]);

// 4. memory
const store = new Map();
const withMemory = new RunnableWithMessageHistory({
  runnable: chain,
  getMessageHistory: (sessionId) => {
    if (!store.has(sessionId)) {
      store.set(sessionId, new InMemoryChatMessageHistory());
    }
    return store.get(sessionId);
  },
  inputMessagesKey: "input",
  historyMessagesKey: "history",
});

// 5. å¤šè½® + æµå¼è¾“å‡º
async function askStream(text) {
  const stream = await withMemory.stream(
    { input: text },
    { configurable: { sessionId: "session1" } }
  );

  console.log("\nAI: ");
  for await (const chunk of stream) {
    if (chunk?.content) {
      process.stdout.write(chunk.content);
    }
  }
  console.log("\n");
}

// ğŸ”¥ æµ‹è¯•å¤šè½®å¯¹è¯ + æµå¼è¾“å‡º
await askStream("ä½ å¥½ï¼Œæˆ‘å«å°è”¡ã€‚");
await askStream("æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆï¼Ÿ");
await askStream("æˆ‘é€‚åˆå­¦ç¼–ç¨‹å—ï¼Ÿ");
