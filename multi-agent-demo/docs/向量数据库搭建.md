明白了，你现在的情况是：

- 有一堆文档（`.doc`, `.pdf`, `.md`, `.docx`, `.xlsx`）存放在 `./data`
- 还没有向量数据库
- Ollama 本地有嵌入模型（`nomic-embed-text`）可以用来生成向量

目标是 **把文档内容向量化**，存储到一个向量数据库（本地也可以是内存或文件存储），然后智能体可以通过 **RAG（Retrieval-Augmented Generation）** 调用这些向量生成内容。

下面我给你一个 **完整可落地的方案**，兼顾你现有的多智能体系统、Ollama 本地嵌入 + Node.js 环境。

---

# 1️⃣ 安装依赖

```bash
npm install pdf-parse mammoth xlsx node-fetch
npm install @types/node --save-dev  # 如果使用 TypeScript
```

- `pdf-parse`：解析 PDF
- `mammoth`：解析 doc / docx
- `xlsx`：解析 Excel
- `node-fetch`：HTTP 请求

---

# 2️⃣ 文档读取模块

`dataLoader.js`：

```js
import fs from "fs";
import path from "path";
import pdfParse from "pdf-parse";
import mammoth from "mammoth";
import xlsx from "xlsx";

export async function loadDocuments(folder = "./data") {
  const files = fs.readdirSync(folder);
  const docs = [];

  for (const file of files) {
    const ext = path.extname(file).toLowerCase();
    const fullPath = path.join(folder, file);

    let content = "";
    if (ext === ".pdf") {
      const dataBuffer = fs.readFileSync(fullPath);
      const pdf = await pdfParse(dataBuffer);
      content = pdf.text;
    } else if (ext === ".doc" || ext === ".docx") {
      const result = await mammoth.extractRawText({ path: fullPath });
      content = result.value;
    } else if (ext === ".md" || ext === ".txt") {
      content = fs.readFileSync(fullPath, "utf-8");
    } else if (ext === ".xlsx") {
      const workbook = xlsx.readFile(fullPath);
      workbook.SheetNames.forEach((sheetName) => {
        const sheet = workbook.Sheets[sheetName];
        content += xlsx.utils.sheet_to_csv(sheet) + "\n";
      });
    }

    if (content.trim()) {
      docs.push({ id: file, content });
    }
  }
  return docs;
}
```

---

# 3️⃣ 使用 Ollama 生成嵌入向量

`embedding.js`：

```js
import fetch from "node-fetch";

/**
 * 使用 Ollama 本地嵌入模型 nomic-embed-text 生成向量
 * @param {string} text
 * @param {string} model
 * @returns {Promise<number[]>}
 */
export async function embedText(text, model = "nomic-embed-text") {
  const response = await fetch(`http://localhost:11434/embed/${model}`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ text }),
  });
  const data = await response.json();
  // 假设返回 { embedding: [...] }
  return data.embedding;
}
```

> 注：你本地 Ollama 的嵌入 API 地址可能需要确认，`11434` 是示例端口，实际按你的 Ollama 配置。

---

# 4️⃣ 构建向量知识库（Memory + 向量存储）

为了简单，可以先用 **内存 + JSON 文件持久化**：

`knowledgeBase.js`：

```js
import fs from "fs";
import path from "path";
import { embedText } from "./embedding.js";

const VECTOR_DB_PATH = "./vector_db.json";

export class KnowledgeBase {
  constructor() {
    this.vectors = []; // [{id, content, embedding}]
  }

  async build(docs) {
    for (const doc of docs) {
      const emb = await embedText(doc.content);
      this.vectors.push({ id: doc.id, content: doc.content, embedding: emb });
    }
    // 持久化
    fs.writeFileSync(VECTOR_DB_PATH, JSON.stringify(this.vectors, null, 2));
  }

  load() {
    if (fs.existsSync(VECTOR_DB_PATH)) {
      this.vectors = JSON.parse(fs.readFileSync(VECTOR_DB_PATH, "utf-8"));
    }
  }

  // 余弦相似度
  cosineSim(a, b) {
    const dot = a.reduce((sum, val, i) => sum + val * b[i], 0);
    const normA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
    const normB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
    return dot / (normA * normB + 1e-10);
  }

  async query(text, topK = 3) {
    const queryVec = await embedText(text);
    const results = this.vectors
      .map((v) => ({ ...v, score: this.cosineSim(queryVec, v.embedding) }))
      .sort((a, b) => b.score - a.score)
      .slice(0, topK);
    return results.map((r) => r.content).join("\n");
  }
}
```

---

# 5️⃣ 使用示例

```js
import { loadDocuments } from "./dataLoader.js";
import { KnowledgeBase } from "./knowledgeBase.js";

async function main() {
  const docs = await loadDocuments("./data");

  const kb = new KnowledgeBase();
  await kb.build(docs); // 生成向量库
  kb.load(); // 载入向量库

  const query = "智能体应用开发流程";
  const context = await kb.query(query);
  console.log("检索到的知识库内容：\n", context);
}

main();
```

---

# 6️⃣ 在 Agent 中使用 RAG

```js
import { KnowledgeBase } from "./knowledgeBase.js";

const kb = new KnowledgeBase();
kb.load();

const topic = "智能体应用开发流程";
const context = await kb.query(topic);

const prompt = `
根据以下知识库信息写文章：
${context}
`;

const result = await runModel({
  model: "ollama:qwen2.5",
  prompt,
  streaming: true,
});
```

这样 **所有文档都能被向量化并用作 RAG**，Agent 可以直接从知识库中检索相关内容增强生成。

---

# 7️⃣ 优化建议

1. **分段嵌入**
   - 如果 PDF/Word 很长，可按段落或每 500\~1000 字生成向量
   - 查询时可以返回 topK 段落，提高召回精度
2. **缓存向量**
   - 每次向量生成耗时，生成完可以写入 `vector_db.json`
   - 后续直接读取，无需每次计算
3. **Agent 集成**
   - 在 `writerAgent` 或 `reviewerAgent` 内调用 `kb.query(topic)`
   - 将 RAG 文本加入 LLM prompt
4. **结合自动工具调用**
   - RAG 检索结果也可以作为工具调用结果的 context

---

✅ 结论：

- 通过 Ollama 的 `nomic-embed-text` 可以生成向量
- 文档格式多样（pdf/doc/md/xlsx）可全部解析
- 使用内存+JSON 持久化做向量知识库
- Agent 可以在 RAG 模式下调用，增强生成效果

---

如果你需要，我可以帮你把这个 **RAG 知识库功能直接整合到现有多智能体项目中**，同时支持 **流式输出 + 插件工具自动调用 + 内部记忆 + Scheduler**，形成一个 **可运行的全功能 multi-agent-demo 项目**。

你希望我直接帮你整合吗？
